{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzFPeD6XLYc9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucbnlp24/hws4nlp24/blob/main/HW3/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZ2oE4GmdfF"
      },
      "source": [
        "# Homework 3: Language Models, Contextual Embedding and BERT\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will explore BERT and measure perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0EWMnOmgWw"
      },
      "source": [
        "##Set Up\n",
        "\n",
        "If you're opening this Notebook on colab, you will probably need to install Transformers. Make sure your version of Transformers is at least 4.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svbBi3YGmMJX"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhnQ3PoVnH7D"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEDSQsLnbLm"
      },
      "source": [
        "IMPORTANT: For this assignment, GPU is not necessary. The following code block should show \"Running on cpu\".\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > None if otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7osYx6Hm0vh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(159259)\n",
        "torch.cuda.manual_seed(159259)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4XlcOvIHgl"
      },
      "source": [
        "# Masking\n",
        "\n",
        "One of the core ideas to wrap your head around with transformer-based language models (and PyTorch) is the concept of *masking*---preventing a model from seeing specific tokens in the input during training.\n",
        "\n",
        "* BERT training relies on the concept of *masked language modeling*: masking a random set of input tokens in a sequence and attempting to predict them.  Remember that BERT is *bidirectional*, so that it can use all of the other non-masked tokens in a sentence to make that prediction.\n",
        "\n",
        "* The GPT class of models acts as a traditional left-to-right language model (sometimes called a \"causal\" LM) .  This family also uses self-attention based transformers---but, when making a prediction for the word $w_i$ at position $i$, it can only use information about words $w_1, \\ldots, w_{i-1}$ to do so.  All of the other tokens following position $i-1$ must be *masked* (hidden from view).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0LW-vKZnIH"
      },
      "source": [
        "Think about a mask as a matrix that's applied to every input $w$ when generating an output $o$ that determines whether an given $o_i$ is allowed to access each token in $w$.  For example, when passing a three-word input sequence through a transformer (to yield a three-word output sequence), a mask is a $3 \\times 3$ matrix where the cells are essentially  answering the following questions:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "o_1 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_2 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_3 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "In the masks we will consider below, 1 denotes that a position should be hidden; 0 denotes that it should be visible. Consider this mask:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "And consider this sequence:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "\\textrm{John} & \\textrm{likes}  & \\textrm{dogs}  \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "When applying this mask to that sequence, we're saying that when we're generating the output for $o_1$ (*John*), we can only consider $w_1$ as an input (*John*).  Likewise, when we generate the output for $o_2$ (*likes*), we can only consider $w_2$ as an input (*likes*), and so on.  (This is a terrible mask!  But illustrates what function a mask performs.)\n",
        "\n",
        "The following code illustrates how this works for that particular mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea2E6zlMZkzO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(159259)\n",
        "\n",
        "def visualize_masking(sequences, mask):\n",
        "  print(mask)\n",
        "  for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "      visible=[]\n",
        "      for j in range(len(sequence)):\n",
        "        if mask[i][j]==0:\n",
        "          visible.append(sequence[j])\n",
        "      print(\"for word %s, the following tokens are visible: %s\" % (sequence[i], visible))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xROu5KmvKMua"
      },
      "outputs": [],
      "source": [
        "sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\n",
        "\n",
        "seq_length=len(sequences[0])\n",
        "\n",
        "test_mask=np.ones((seq_length,seq_length))\n",
        "for i in range(seq_length):\n",
        "  test_mask[i,i]=0\n",
        "\n",
        "visualize_masking(sequences, test_mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv3h625eMo3"
      },
      "source": [
        "##Q1.  \n",
        "As we discussed in class, BERT masks a random set of words in the input and attempts to reconstruct those words as output.  **Create a mask that always masks token positions 2 and 7 for a size 10 sequence input.** You should generate output representations for all 10 tokens (i.e., $[o_1, \\ldots, o_{10}]$ in the notation above. Each representation must ignore input tokens at position 2 and 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_31mkhIke-YI"
      },
      "outputs": [],
      "source": [
        "def create_bert_mask(seq_length):\n",
        "  mask=np.ones((seq_length,seq_length))\n",
        "  # implement BERT mask here\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivKIWJCObyOe"
      },
      "outputs": [],
      "source": [
        "visualize_masking(sequences, create_bert_mask(len(sequences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKjasreUfLww"
      },
      "source": [
        "##Q2\n",
        "A left-to-right language model (such as GPT) can only use information from input words $[w_1, \\ldots, w_{i}]$ when generating the representation for output $o_i$.  **Encode a representation that masks input from $[w_{i+1}, \\ldots, w_{n}]$.**\n",
        "\n",
        "*Example:*\n",
        "\n",
        "For input of length 5, the beginning of the mask should be:\n",
        "\n",
        "```\n",
        "[[0, 1, 1, 1, 1],\n",
        " [0, 0, 1, 1, 1],\n",
        " ...             ]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svKuQyQff4fK"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_length):\n",
        "  mask=np.ones((seq_length,seq_length))\n",
        "  # implement causal mask here\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwOwAP37bG6d"
      },
      "outputs": [],
      "source": [
        "visualize_masking(sequences, create_causal_mask(len(sequences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRkI2DZnwvB"
      },
      "source": [
        "Now let's go ahead and embed these masks within a model.  First, we'll load some textual data (from Austen's *Pride and Prejudice*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgTwRcqDn6ni"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v2EJeEBobD8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8hjeySdojyr"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jre3sSWgvnwi"
      },
      "source": [
        "Let's read in the data and tokenize it; for this homework, we'll only work with the first 10,000 tokens of that book; we'll keep only the most frequent 1,000 word types (all other tokens will be mapped to an [UNK] token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kzw9IcFoDZq"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "  with open(filename) as file:\n",
        "    data=file.read().lower()\n",
        "    first10K=' '.join(data.split(\" \")[:10000])\n",
        "    toks=nltk.word_tokenize(first10K)[:10000]\n",
        "    vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "    counts=Counter()\n",
        "    for tok in toks:\n",
        "      counts[tok]+=1\n",
        "    for v, _ in counts.most_common(1000):\n",
        "      vocab[v]=len(vocab)\n",
        "    tokids=[]\n",
        "    for tok in toks:\n",
        "      tokid=1\n",
        "      if tok in vocab:\n",
        "        tokid=vocab[tok]\n",
        "\n",
        "      tokids.append(tokid)\n",
        "\n",
        "    return tokids, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_XHBZPv4kD"
      },
      "source": [
        "Now let's specify our model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p74L0s1DsKa2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MaskedLM(nn.Module):\n",
        "    def __init__(self, vocab, mask, d_model=512):\n",
        "        super().__init__()\n",
        "        self.vocab=vocab\n",
        "        self.mask=mask\n",
        "        vocab_size=len(vocab)\n",
        "        self.embeddings=nn.Embedding(1002,512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear=torch.nn.Linear(d_model, vocab_size)\n",
        "        self.rev_vocab={vocab[k]:k for k in vocab}\n",
        "\n",
        "    def forward(self, input):\n",
        "        # first we pass the input word IDS through an embedding layer to get embeddings for them\n",
        "        input=self.embeddings(input)\n",
        "        # then we pass those embeddings through a transformer to get contextual representations, masking the input where appropriate\n",
        "        out = self.transformer_encoder.forward(input, mask=self.mask)\n",
        "        # finally we pass those embeddings through a linear layer to transform it into the output space (the size of our vocabulary)\n",
        "        h=self.linear(out)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njdPnGVQsQOl"
      },
      "outputs": [],
      "source": [
        "def get_batches(xs, ys, batch_size=32):\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    for i in range(0, len(xs), batch_size):\n",
        "        batch_x.append(torch.LongTensor(xs[i:i+batch_size]).to(device))\n",
        "        batch_y.append(torch.LongTensor(ys[i:i+batch_size]).to(device))\n",
        "    return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm6tLzHmtAfN"
      },
      "outputs": [],
      "source": [
        "tokids, vocab=read_data(\"1342-0.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9822mHI3sR1Q"
      },
      "outputs": [],
      "source": [
        "def train(mask, data_function, tokids, vocab):\n",
        "\n",
        "    mask=torch.BoolTensor(mask).to(device)\n",
        "\n",
        "    num_labels=len(vocab)\n",
        "    model=MaskedLM(vocab, mask).to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "    losses=[]\n",
        "\n",
        "    xs, ys=data_function(tokids)\n",
        "\n",
        "    batch_x, batch_y=get_batches(xs, ys)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "\n",
        "        for x, y in list(zip(batch_x, batch_y)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred=model.forward(x)\n",
        "            loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            print(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CXKNwLBwBG6"
      },
      "source": [
        "Our model and training process are now all defined; all that remains is to pass our inputs and outputs through it to train.  Your job here is to create the correct inputs (x) and outputs (y) to train a left-to-right (causal) language model.\n",
        "\n",
        "##Q3\n",
        "**Write a function that takes in a sequence of token ids $[w_1, \\ldots, w_n]$ and segments it into 8-token chunks and creates a corresponding $y_i$ for every $x_i$.** The x segmentation should operate like this: $x_1=[w_1, \\ldots, w_8]$, $x_2=[w_9, \\ldots, w_{16}]$, etc. Given this language modeling specification, each $y_i$ should also contain 8 values (for each token in $x_i$).  Keep in mind this is a left-to-right causal language model; your job is to figure out the values of y that respects this design. At token position $i$, when a model has access to $[w_1, \\ldots, w_i]$, which is the true $y_i$ for that position? Each element in $y$ should be a word ID (i.e., an integer).\n",
        "\n",
        "Edge Case: For the case when the length of `data` is not divisilble by `max_len`, we will discard the remainder part of the input data. Here is an example for clarity:\n",
        "\n",
        " >  Given this sentence input:  \n",
        " > \"This is a sentence for natural language processing homework that goes on and on\"   \n",
        " > The function should produce this segmentation (with `max_len = 8`):  \n",
        " > `xs = [[This, is, a, sentence, for, natural, language, processing]]`  \n",
        " > The similar applies to `ys`\n",
        "\n",
        " *Note*: `ys` will not be a copy of `xs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIO8M9AeszAi"
      },
      "outputs": [],
      "source": [
        "def get_causal_xy(data, max_len=8):\n",
        "    xs=[]\n",
        "    ys=[]\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO4WOusZsWsC"
      },
      "outputs": [],
      "source": [
        "seq_length=8\n",
        "\n",
        "train(create_causal_mask(seq_length=seq_length), get_causal_xy, tokids, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJv2fHphaluE"
      },
      "source": [
        "*You should be seeing the cross entropy loss gradually decrease to be less than 5.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZpVrqchSAI"
      },
      "source": [
        "# Perplexity\n",
        "To evaluate how good our language model is, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$\n",
        "\n",
        "Here we want to calculate the perplexity of [pretrained BERT model](https://huggingface.co/bert-base-uncased) on text from different sources. When calculating perplexity with BERT, we'll use a related measure of pseudo-perplexity, which allow us to condition on the bidirectional context (and not just the left context, as in standard perplexity):\n",
        "\n",
        "$$PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i} \\mid w_{1}\\dots w_{i - 1}, w_{i+1}, \\ldots, w_n)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2MXSfzlsHp_"
      },
      "source": [
        "First, let's instantiate a BERT model, along with its WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxyJaS1-cP3R"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "base_model_name = 'bert-base-uncased'\n",
        "base_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model=base_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLAWTU8sTqf"
      },
      "source": [
        "Let's see how the BERT tokenizer tokenizes a sentence into a sequence of WordPiece ids.  Note how BERT tokenization automatically wraps an input sentences with [CLS] and [SEP] tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxlOsB0NshJm"
      },
      "outputs": [],
      "source": [
        "sentence = \"A dog landed on Mars\"\n",
        "tensor_input = base_tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tensor_input)\n",
        "tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "print(tensor_input_ids)\n",
        "print(base_tokenizer.convert_ids_to_tokens(tensor_input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGu38_DB1vsB"
      },
      "source": [
        "Now let's see how we can calculate output probabilities using this model.  The output of each token position $i$ gives us $P(w_i \\mid w_1, \\ldots, w_n)$---the probability of the word at that position over our vocabulary, given *all* of the words in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvuNw52m15OU"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  output = base_model(tensor_input_ids)\n",
        "  logits = output.logits\n",
        "  # logits here are the unnormalized scores, so let's pass them through the softmax\n",
        "  # to get a probability distribution\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  # for one input sequence, the shape of the resulting distribution is:\n",
        "  # 1 x [length of input, in WordPiece tokens] x (the size of the BERT vocabulary)\n",
        "  print(softmax.shape) # [1, 7, 30522]\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  # Let's print the probability of the true inputs\n",
        "  wp_tokens=base_tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  for i in range(len(input_ints)):\n",
        "    prob=softmax[0][i][input_ints[i]].numpy()\n",
        "    print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F90uSrL_1vot"
      },
      "source": [
        "Note that $w_i$ is in the range $[w_1, \\ldots, w_n]$ -- clearly the probability of a word is going to be high when we can observe it in the input! Let's do some masking to calculate $P(w_i \\mid w_1, \\ldots w_{i-1}, w_{i+1}, w_n)$.  Now annoyingly, BERT's `attention_mask` function only works for padding tokens; to mask input tokens, we need to intervene in the input and replace a WordPiece token that we're predicting with a special [MASK] token (BERT tokenizer word id `103`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77EdDCtk3MSa"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "with torch.no_grad():\n",
        "  # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "  masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "  # we'll mask the second word\n",
        "  masked_input_ids[0][1]=base_tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "  print(\"The second word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "\n",
        "  # now let's run that through BERT in the same way we did before\n",
        "  output = base_model(masked_input_ids)\n",
        "  logits = output.logits\n",
        "\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "\n",
        "  wp_tokens=base_tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  i=1\n",
        "  prob=softmax[0][i][input_ints[i]].numpy()\n",
        "  print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdj7onm1vln"
      },
      "source": [
        "You can see the probability of \"a\" as the second token has gone down to 0.13965 when we mask it.  This is the $P(w_1 =\\textrm{a} \\mid w_0, w_2, \\ldots, w_n)$.  At this point you should have everything you need to calculate the BERT pseudo-perplexity of an input sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_eHLic_OfM"
      },
      "source": [
        "##Q4\n",
        "**Implement the pseudo-perplexity measure described above, calculating the perplexity for a given model, tokenizer, and sentence. You MUST comment your code to clarify the steps you had taken.**\n",
        "\n",
        "The function calculates the average probability of each token in the sentence given all the other tokens. We need to predict the probability of each word in a sentence by masking the one word to predict. Note that you should not include the probabilities of the [CLS] and [SEP] tokens in your perplexity equation -- those tokens are not part of the original test sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDZ9Qe8L-uUt"
      },
      "outputs": [],
      "source": [
        "# This function calculates the perplexity of a language model, given a sentence and its corresponding tokenizer\n",
        "\n",
        "# Inputs:\n",
        "# model: language model being used to calculate the perplexity\n",
        "# tokenizer: tokenizer that is used to preprocess the input sentence\n",
        "# sentence: input sentence string for which perplexity is to be calculated\n",
        "\n",
        "# Outputs:\n",
        "# returns perplexity of the input sentence\n",
        "\n",
        "def perplexity(model, tokenizer, sentence):\n",
        "\n",
        "    # hints: you'll need to:\n",
        "    # encode the input sentence using the tokenizer\n",
        "    # for each WordPiece token in the sentence (except [CLS] and [SEP]), mask that single token and\n",
        "    # calculate the probability of that true word at the masked position\n",
        "    # don't calculate perplexity for the [CLS] and [SEP] tokens (which are not part of the original test sentence).\n",
        "\n",
        "    perplexity=None\n",
        "    # BEGIN SOLUTION\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN7HVo_oIWXF"
      },
      "outputs": [],
      "source": [
        "print(perplexity(sentence='London is the capital of the United Kingdom.', model=base_model, tokenizer=base_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ooey1BAezPO"
      },
      "source": [
        "*Sanity Check: You should be seeing perplexity close to 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqvqVR26H_Ny"
      },
      "source": [
        "# Comparing Different Pretrained BERT using Perplexity\n",
        "\n",
        "Note: the following section will be using your implementation of `perplexity()`.\n",
        "\n",
        "In a previous question, we explored the perplexity of `bert-base-uncased` using the sentence \"London is the capital of the United Kingdom.\" According to its [HuggingFace page](https://huggingface.co/bert-base-uncased), this model underwent pretraining with two significant datasets: **BookCorpus**—comprising 11,038 unpublished books—and **English Wikipedia**, specifically its text portions (lists, tables, and headers excluded). It's important to note that both datasets are in English. Given this background, it would be insightful to assess the model's performance with a Simplified Chinese translation of the sentence \"London is the capital of the United Kingdom,\" considering its training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY65tRSla7Vf"
      },
      "outputs": [],
      "source": [
        "# Simplified Chinese translation of \"London is the capital of the United Kingdom.\"\n",
        "print(perplexity(sentence='伦敦是英国的首都。', model=base_model, tokenizer=base_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskN9CPkd7iV"
      },
      "source": [
        "Luckily, we have another flavor of BERT just for that. [**BERT Multilingual**](https://huggingface.co/bert-base-multilingual-uncased) is pretrained on the 102 languages with the largest Wikipedias. This way, the model learn inner representation of multiple languages. Now, let's see how it will perform on the previous example in Chinese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnLs4hk1aHzE"
      },
      "outputs": [],
      "source": [
        "multilingual_model_name = ... # TODO: Click on the Hugging Face link and fill in the model name\n",
        "multilingual_model = AutoModelForMaskedLM.from_pretrained(multilingual_model_name)\n",
        "multilingual_tokenizer = AutoTokenizer.from_pretrained(multilingual_model_name)\n",
        "multilingual_model=multilingual_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsaHeA4fhd2-"
      },
      "outputs": [],
      "source": [
        "# Simplified Chinese translation of \"London is the capital of the United Kingdom.\"\n",
        "print(perplexity(sentence='伦敦是英国的首都。', model=multilingual_model, tokenizer=multilingual_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdvpSSa-hmoJ"
      },
      "source": [
        "The third model in our discussion, [**BERT Large**](https://huggingface.co/bert-large-uncased), significantly expands on its predecessors in terms of size and capacity. While [**BERT Base**](https://huggingface.co/bert-base-uncased) contains 110 million parameters, and [**BERT Multilingual**](https://huggingface.co/bert-base-multilingual-uncased) slightly increases this figure to 168 million parameters, **BERT Large** boasts an impressive 336 million parameters. This substantial increase in parameters enables the **BERT Large** model to handle more complex language tasks with greater efficacy. However **BERT Large** is pretrained on the same dataset as **BERT Base**. In the following question, you will compare the three BERT models side by side using multilingual inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS-pHwhAaLG9"
      },
      "outputs": [],
      "source": [
        "large_model_name = ... # TODO: Click on the Hugging Face link and fill in the model name\n",
        "large_model = AutoModelForMaskedLM.from_pretrained(large_model_name)\n",
        "large_tokenizer = AutoTokenizer.from_pretrained(large_model_name)\n",
        "large_model=large_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyBKuEbkmzEZ"
      },
      "source": [
        "## Q5 (Write up)\n",
        "\n",
        "We will evaluate the performance of three BERT models by calculating their perplexity across various inputs. These inputs will differ along two dimensions:\n",
        "\n",
        "1. Logical coherence (Rational vs. Irrational)\n",
        "2. Language (English vs. Other languages)\n",
        "\n",
        "Below are the four sample inputs provided for this analysis:\n",
        "\n",
        "- For logical coherence in English:\n",
        "  - Rational: \"London is the capital of the United Kingdom\"...\n",
        "  - Irrational: \"London is the capital of the United States\"...\n",
        "  \n",
        "- For logical coherence in Simplified Chinese:\n",
        "  - Rational: \"伦敦是英国的首都。\"... (London is the capital of the United Kingdom.)\n",
        "  - Irrational: \"伦敦是美国的首都。\"... (London is the capital of the United States.)\n",
        "\n",
        "The goal is to compute and compare the perplexity of our BERT models on these inputs. This comparison will help us understand how well each model can handle logical coherence and language variability. After running the two cells below, you will write a short answer in response to the results.\n",
        "\n",
        "*Note: we are computing the average perplexity over 10 example sentences for each style of input. This is because, practically, perplexity evaluation of LMs is done through averaging it on a corpus, as the individual sentence scores can have random variations.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VraPW0GID3h"
      },
      "outputs": [],
      "source": [
        "def compare_models(inputs: list[list[str]], model_dictionary: dict):\n",
        "  for input in inputs:\n",
        "    print(input[0] + \"...\")\n",
        "    for model_name, model_meta in model_dictionary.items():\n",
        "      ps = []\n",
        "      for sentence in input:\n",
        "        p = perplexity(\n",
        "          sentence=sentence,\n",
        "          model=model_meta[\"model\"],\n",
        "          tokenizer=model_meta[\"tokenizer\"]\n",
        "        )\n",
        "        ps.append(p)\n",
        "      avg_p = np.mean(ps)\n",
        "      print(f\" - {model_name}\\t{avg_p}\")\n",
        "    print(\"==============\")\n",
        "\n",
        "english_rational = [\n",
        "    \"London is the capital of the United Kingdom.\",\n",
        "    \"The Great Wall of China is visible from space.\",\n",
        "    \"Shakespeare wrote the play 'Romeo and Juliet'.\",\n",
        "    \"The human body has 206 bones.\",\n",
        "    \"The Earth orbits the Sun once every 365.25 days.\",\n",
        "    \"The chemical symbol for gold is Au.\",\n",
        "    \"The Pacific Ocean is the largest ocean on Earth.\"\n",
        "]\n",
        "english_irrational = [\n",
        "    \"London the is United States capital the of.\",\n",
        "    \"longest The Great slideof China Wall in the. world is the\",\n",
        "    \"screenplay Shakespeare the for 'wrote The Terminator'.\",\n",
        "    \"body has 206 The human pebbles.\",\n",
        "    \"The Moon once 365.25 Earth orbits every the days.\",\n",
        "    \"The symbol chemical for  Gl gold is  .\",\n",
        "    \"The Pacific Ocean is the largest swimming pool on Earth.\"\n",
        "]\n",
        "chinese_rational = [\n",
        "    \"伦敦是英国的首都。\",\n",
        "    \"长城可以从太空中看见。\",\n",
        "    \"莎士比亚写了《罗密欧与朱丽叶》这部戏剧。\",\n",
        "    \"人体有206块骨头。\",\n",
        "    \"地球绕太阳转一圈大约需要365.25天。\",\n",
        "    \"金的化学符号是Au。\",\n",
        "    \"太平洋是地球上最大的海洋。\"\n",
        "]\n",
        "chinese_irrational = [\n",
        "    \"伦敦的首是美国都。\",\n",
        "    \"长滑梯城上最长是世界的。\",\n",
        "    \"《终结者》了的剧本莎士比亚写。\",\n",
        "    \"人体颗鹅卵有206石。\",\n",
        "    \"地球365.25一圈大约需要天绕月球转。\",\n",
        "    \"化学符号金Gl的是。\",\n",
        "    \"最大太平洋上的是地球游泳池。\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMrYUcBNNC3i"
      },
      "outputs": [],
      "source": [
        "model_dictionary = {\n",
        "    \"Base BERT\" : {\n",
        "        \"model\" : base_model,\n",
        "        \"tokenizer\" : base_tokenizer\n",
        "    },\n",
        "    \"Multilingual\" : {\n",
        "        \"model\" : multilingual_model,\n",
        "        \"tokenizer\" : multilingual_tokenizer\n",
        "    },\n",
        "    \"Large BERT\" : {\n",
        "        \"model\" : large_model,\n",
        "        \"tokenizer\" : large_tokenizer\n",
        "    }\n",
        "}\n",
        "\n",
        "inputs = [\n",
        "    english_rational,\n",
        "    english_irrational,\n",
        "    chinese_rational, # Translation of `english_rational`\n",
        "    chinese_irrational  # Translation of `english_irrational`\n",
        "]\n",
        "\n",
        "compare_models(inputs, model_dictionary)\n",
        "\n",
        "# THIS CELL WILL TAKE ~5 MIN TO RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8giF1jFsCru"
      },
      "source": [
        "**Observe the results and highlight patterns that reflect characteristics of the BERT models. For each observation, use the observed pattern to justify characteristics of one specific model or a group of models. You should provide 3 observations with 50 words each**. One example is provided for you. Please double click the cell below to enter your response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQcHvYzRaApo"
      },
      "source": [
        "1. (Example) Perplexity of Base BERT and Large BERT both increase on irrational English sentences, compared to rational Englsih sentences. This justifies that both models are better at prediciting rational sentences due to knowledge learned from their pretrained text corpus.\n",
        "2. ...\n",
        "3. ...\n",
        "4. ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}